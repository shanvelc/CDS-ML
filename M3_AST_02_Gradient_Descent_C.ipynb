{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanvelc/CDS-ML/blob/main/M3_AST_02_Gradient_Descent_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alien-mercury"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 2: Gradient Descent"
      ],
      "id": "alien-mercury"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pending-kennedy"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "pending-kennedy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noble-feature"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* understand various optimization algorithms\n",
        "* minimize the value of cost function by finding the minima\n",
        "* implement gradient descent algorithm and its variations: stochastic, batch and mini-batch gradient descent"
      ],
      "id": "noble-feature"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Walkthrough Video\n",
        "from IPython.display import HTML\n",
        "HTML(\"\"\"<video width=\"420\" height=\"240\" controls>\n",
        "<source src=\"https://cdn.chn.talentsprint.com/content/Gradient_Descent_v1_Debasish_Bhaskar_Updated.mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4f0MGvdAvLLM"
      },
      "id": "4f0MGvdAvLLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "commercial-operation"
      },
      "source": [
        "## Information"
      ],
      "id": "commercial-operation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79hZiGrpW-l"
      },
      "source": [
        "#### Optimization:\n",
        "\n",
        "Optimization consists of procedures that make a system as effective as possible. It is an important tool in decision science and in the analysis of physical systems. To make use of this tool, we must first identify some objective, a quantitative measure of the performance of the system under study. This objective could be profit, time, potential energy, or any quantity or combination of quantities that can be represented by a single number. The objective depends on certain characteristics of the system, called variables or unknowns. Our goal is to find values of the variables that optimize the objective.\n",
        "\n",
        "* Helps improve the quality of decision-making\n",
        "* Applications in Engineering, Business, Economics, Science, Military Planning etc.\n",
        "\n",
        "**Gradient Descent**: It is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function J(w) w.r.t the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate $Î±$. Therefore, we follow the direction of the slope downhill until we reach a local minimum (refer to the image below).\n",
        "![img](https://miro.medium.com/max/700/1*rcmvCjQvsxrJi8Y4HpGcCw.png)"
      ],
      "id": "d79hZiGrpW-l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ],
      "id": "BNLA8HiKxQhc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "id": "2YzfoPvJDiTX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "id": "AjoZJWGErxGf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_02_Gradient_Descent_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx wget -qq https://cdn.iisc.talentsprint.com/CDS/Datasets/SIMPLEPENDULUMOSCILLATIONDATA.txt\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "id": "WBPPuGmBlDIN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "everyday-cotton"
      },
      "source": [
        "#### Importing required packages"
      ],
      "id": "everyday-cotton"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "likely-delay"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import scipy.optimize as opt\n",
        "import pandas as pd"
      ],
      "id": "likely-delay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-QqeVbF1t7Y"
      },
      "source": [
        "### Optimization"
      ],
      "id": "m-QqeVbF1t7Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "significant-truth"
      },
      "source": [
        " First, let's define a simple mathematical function (the opposite of the cardinal sine). This function has many local minima but a single global minimum.\n",
        "\n",
        " Refer the [link](https://www.mathworks.com/help/signal/gs/the-sinc-function.html) for more information on the below function."
      ],
      "id": "significant-truth"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hollow-vatican"
      },
      "source": [
        "# function f\n",
        "def f(x):\n",
        "    return 1 - np.sin(x) / x"
      ],
      "id": "hollow-vatican",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "behind-sunset"
      },
      "source": [
        "Let's plot this function in the interval [â20,20] (with 1000 samples)"
      ],
      "id": "behind-sunset"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unique-dancing"
      },
      "source": [
        "# create an array and apply the function\n",
        "x = np.linspace(-20., 20., 1000)\n",
        "y = f(x)\n",
        "# visualize the plot\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "ax.plot(x, y)"
      ],
      "id": "unique-dancing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "polyphonic-distinction"
      },
      "source": [
        "The `scipy.optimize` module comes with many function minimization routines. The `minimize()` function offers a unified interface to many algorithms. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm (the default algorithm in `minimize()`) gives good results in general. The `minimize()` function requires an initial point as argument. For scalar univariate functions, we can also use `minimize_scalar()`"
      ],
      "id": "polyphonic-distinction"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broke-posting"
      },
      "source": [
        "# optimize\n",
        "x0 = 3\n",
        "xmin = opt.minimize(f, x0).x"
      ],
      "id": "broke-posting",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "forbidden-arrest"
      },
      "source": [
        "Starting from x0=3, the algorithm was able to find the actual global minimum, as shown in the following figure"
      ],
      "id": "forbidden-arrest"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brief-advertising"
      },
      "source": [
        "# visualize\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "ax.plot(x, y)\n",
        "ax.scatter(x0, f(x0), marker='o', s=300)\n",
        "ax.scatter(xmin, f(xmin), marker='v', s=300, zorder=20)\n",
        "ax.set_xlim(-20, 20)"
      ],
      "id": "brief-advertising",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liked-accordance"
      },
      "source": [
        "Now, if we start from an initial point that is farther away from the actual global minimum, the algorithm converges towards a local minimum only"
      ],
      "id": "liked-accordance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "circular-singles"
      },
      "source": [
        "# opitmize from initial point\n",
        "x0 = 10\n",
        "xmin = opt.minimize(f, x0).x"
      ],
      "id": "circular-singles",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "integrated-wyoming"
      },
      "source": [
        "# visualize\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "ax.plot(x, y)\n",
        "ax.scatter(x0, f(x0), marker='o', s=300)\n",
        "ax.scatter(xmin, f(xmin), marker='v', s=300, zorder=20)\n",
        "ax.set_xlim(-20, 20)"
      ],
      "id": "integrated-wyoming",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "patent-mercy"
      },
      "source": [
        "## Gradient descent\n",
        "Gradient descent is an optimization algorithm to find the minimum of a function. We start with a random point on the function and move in the negative direction of the gradient of the function to reach the local/global minima.\n",
        "\n",
        "Gradient descent is the backbone of a machine learning algorithm."
      ],
      "id": "patent-mercy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Find the local minima of the function $y=(x+5$)$^2$ starting from the point $x=3$\n",
        "\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/convex_curve.png\" width = \"350px;\"/>\n"
      ],
      "metadata": {
        "id": "MD3mNx0UPAo_"
      },
      "id": "MD3mNx0UPAo_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "By observing the above plot, we know that $y = (x+5)^2$ reaches itâs minimum value when x = -5 (i.e when x=-5, y=0).\n",
        "\n",
        "Hence x=-5 is the local and global minima of the function.\n",
        "\n",
        "Now we will see how to obtain the same numerically, using gradient descent.\n",
        "\n",
        "**Step 1 :** Initialize x =3. Find the gradient of the function, dy/dx = 2*(x+5).\n",
        "\n",
        "**Step 2 :** Move in the direction of the negative of the gradient. The learning rate determines how many steps to move. Let us assume the learning rate is 0.01\n",
        "\n",
        "**Step 3 :** Perform 2 iterations of gradient descent\n",
        "\n",
        "The parameter update formula in gradient descent is:\n",
        "\n",
        "$ x_{new} = x_{old} - (learning\\ rate * (dy/dx))$,\n",
        "\n",
        "- where $dy/dx$ is the derivative of the function with respect to a single weight, done for all the weights\n",
        "\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/GD_iterations.png\" width =\"480px;\"/>\n",
        "\n",
        "**Step 4 :** We can observe that the X value is slowly decreasing and should converge to -5 (the local minima). However, how many iterations should we perform?\n",
        "\n",
        "We can set a precision variable in our algorithm which calculates the difference between two consecutive âxâ values. If the difference between x values from 2 consecutive iterations is lesser than the precision we set, stop the algorithm!"
      ],
      "metadata": {
        "id": "LuKUso64RjgO"
      },
      "id": "LuKUso64RjgO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can solve the above optimization problem by implementing the gradient descent algorithm in Python:"
      ],
      "metadata": {
        "id": "ACP7ro1Bcj1y"
      },
      "id": "ACP7ro1Bcj1y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 : Initialize parameters\n",
        "\n",
        "cur_x = 3                   # The algorithm starts at x=3\n",
        "rate = 0.01                 # Learning rate\n",
        "precision = 0.0001          # This tells us when to stop the algorithm\n",
        "previous_step_size = 1      # Limit to monitor change in x\n",
        "max_iters = 10000           # maximum number of iterations\n",
        "iters = 0                   # iteration counter\n",
        "df = lambda x: 2*(x+5)      # Gradient of our function"
      ],
      "metadata": {
        "id": "3gRWKLvBgbwj"
      },
      "id": "3gRWKLvBgbwj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 : Run a loop to perform gradient descent:\n",
        "# i. Stop loop when difference between x values from last 2 consecutive iterations is less than 0.0001 or when number of iterations exceeds 10,000\n",
        "\n",
        "while previous_step_size > precision and iters < max_iters:\n",
        "    prev_x = cur_x                                     # Store current x value in prev_x\n",
        "    cur_x = cur_x - rate * df(prev_x)                  # Grad descent\n",
        "    previous_step_size = abs(cur_x - prev_x)           # Change in x\n",
        "    iters = iters+1                                    # iteration count\n",
        "    print(\"Iteration\", iters, \"\\nX value is\", cur_x)   # Print iterations\n",
        "\n",
        "print(\"The local minimum occurs at\", cur_x)"
      ],
      "metadata": {
        "id": "q2MxtDiHgkQ6"
      },
      "id": "q2MxtDiHgkQ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above output we can observe the $x$ values after each iteration and the optimized solution. The $x$ values from the first 2 iterations can be cross checked with our earlier manual calculation."
      ],
      "metadata": {
        "id": "z5B1t60kikc5"
      },
      "id": "z5B1t60kikc5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diverse-civilian"
      },
      "source": [
        "###Solving a linear regression problem with Gradient descent\n",
        "\n",
        "Here, we consider a simple pendulum oscillation dataset with two variables. The dataset consists of two columns and 89 rows. Each column represents a characteristic of a simple pendulum i.e l (length) and t (time period). The dataset describes the relationship between the l and t which is $LâT^2$ ."
      ],
      "id": "diverse-civilian"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advance-symposium"
      },
      "source": [
        "# Load the data by using pandas read_csv()\n",
        "data = pd.read_csv(\"SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\n",
        "data.head()"
      ],
      "id": "advance-symposium",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "endless-summer"
      },
      "source": [
        "# Get the length and time period values from the dataset\n",
        "l = data['l'].values\n",
        "t = data['t'].values\n",
        "# Get the square of time period\n",
        "tsq = t * t"
      ],
      "id": "endless-summer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "earned-arena"
      },
      "source": [
        "### Batch Gradient Descent\n",
        "\n",
        "Batch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples:\n",
        "\n",
        "$$w = w - \\alpha \\triangledown_w J(w)$$\n",
        "\n",
        "The below function `train()` updates the values of $m$ and $c$ in the linear regression equation $y = mx + c$, and calculates error. The loss is minimized due to the changed values of m and c. The new values m, c and the minimized error are returned."
      ],
      "id": "earned-arena"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tropical-craft"
      },
      "source": [
        "# function to update m and c\n",
        "def train(x, y, m, c, eta):                   # x = independent variable, y = dependent variable, m = coeff. of independent var (slope), c = constant (bias), eta = learning rate\n",
        "    const = - 2.0/len(y)                      # constant term to be used in compting gradients\n",
        "    ycalc = m * x + c                         # linear regression expression\n",
        "    delta_m = const * sum(x * (y - ycalc))    # dy/dm\n",
        "    delta_c = const * sum(y - ycalc)          # dy/dc\n",
        "    m = m - delta_m * eta                     # gradient descent step to update m\n",
        "    c = c - delta_c * eta                     # gradient descent step to update c\n",
        "    error = sum((y - ycalc)**2)/len(y)        # error\n",
        "    return m, c, error"
      ],
      "id": "tropical-craft",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diverse-cross"
      },
      "source": [
        "Let us vary the learning rate and find how the error decreases in each case, and how the final line looks, by training each case for the same number of iterations - 2000."
      ],
      "id": "diverse-cross"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alert-marsh"
      },
      "source": [
        "# Save errors\n",
        "errs_1 = []\n",
        "m, c = 0, 0         # initial guess\n",
        "eta = 0.1           # learning rate\n",
        "\n",
        "# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.1\n",
        "for iteration in range(2000):\n",
        "    m, c, error = train(l, tsq, m, c, eta)\n",
        "    errs_1.append(error)\n",
        "\n",
        "# Save final line\n",
        "m_1, c_1 = m, c\n",
        "m_1, c_1"
      ],
      "id": "alert-marsh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "revolutionary-driving"
      },
      "source": [
        "Visualize the function"
      ],
      "id": "revolutionary-driving"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "proprietary-tradition"
      },
      "source": [
        "# Find the lines\n",
        "y_1 = m_1 * l + c_1\n",
        "plt.plot(l, tsq, '.k');\n",
        "plt.plot(l, y_1, \"g\");\n",
        "plt.xlabel(\"$L$\", fontsize=15)\n",
        "plt.ylabel(\"$T^2$\", fontsize=15)\n",
        "plt.show()"
      ],
      "id": "proprietary-tradition",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mathematical-banner"
      },
      "source": [
        "### Stochastic gradient descent:\n",
        "\n",
        "Instead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example $(x_i,y_i)$. Therefore, learning happens on every example:\n",
        "$$w = w - \\alpha \\triangledown_w J(x_i,y_i;w)$$"
      ],
      "id": "mathematical-banner"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crucial-venezuela"
      },
      "source": [
        "given the equation $y_i = mx_i + c$\n",
        "\n",
        "we calculate the error as $E$ = $(y - y_i)^2$ ; where $y$ is the ground truth and $y_i$ is the prediction\n",
        "\n",
        "Finding the rate of change in error with respect to m is $\\frac{\\partial E_i }{\\partial m}$ = $ -2(y_i - (mx_i + c)) * x_i$\n",
        "\n",
        "rate of change in c is $\\frac{\\partial E_i }{\\partial c}$ = $ -2(y_i - (mx_i + c))$\n",
        "\n",
        "And then we update the slope and bias with change in slope $\\Delta m$ and change in bias $\\Delta c$ with learning rate $eta$\n",
        "\n",
        "$m$  = $m - \\Delta m * eta$\n",
        "\n",
        "$c$  = $c - \\Delta c * eta$"
      ],
      "id": "crucial-venezuela"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sharp-redhead"
      },
      "source": [
        "The below function `next_step()` updates the values of m and c and calculates error. The loss is minimized due to the changed values of m and c. The new values m, c and the minimized loss are returned."
      ],
      "id": "sharp-redhead"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "refined-isaac"
      },
      "source": [
        "# change in m and c\n",
        "def next_step(x, y, m, c, eta):\n",
        "    ycalc = m * x + c                # linear regression expression\n",
        "    error = (y - ycalc) ** 2         # error\n",
        "    delta_m = -(y - ycalc) * x       # dy/dm\n",
        "    delta_c = -(y - ycalc)           # dy/dc\n",
        "    m = m - delta_m * eta            # gradient descent step to update m\n",
        "    c = c - delta_c * eta            # gradient descent step to update c\n",
        "    return m, c, error"
      ],
      "id": "refined-isaac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "front-guess"
      },
      "source": [
        "The function below takes a random data point at a time and updates m and c using the function `next_step()`"
      ],
      "id": "front-guess"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "overhead-humor"
      },
      "source": [
        "# update m and c for one data point\n",
        "def one_loop_random(x, y, m, c, eta):\n",
        "    # Making random idx\n",
        "    random_idx = np.arange(len(y))\n",
        "    np.random.shuffle(random_idx)\n",
        "    # Training with random idx\n",
        "    for idx in random_idx:\n",
        "        m, c, e = next_step(x[idx], y[idx], m, c, eta)\n",
        "        #print(m, c, e)\n",
        "    return m,c,e"
      ],
      "id": "overhead-humor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brave-western"
      },
      "source": [
        "The function below trains the data for 1000 iterations. In each iteration it calls the `one_loop_random()` function."
      ],
      "id": "brave-western"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bright-cabinet"
      },
      "source": [
        "# train for 1000 iterations\n",
        "def train_stochastic(x, y, m, c, eta, iterations=1000):\n",
        "    for iteration in range(iterations):\n",
        "        m, c, err = one_loop_random(x, y, m, c, eta)\n",
        "    return m, c, err"
      ],
      "id": "bright-cabinet",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coastal-georgia"
      },
      "source": [
        "# Initialize m, c\n",
        "m, c = 0, 0\n",
        "# Learning rate\n",
        "lr = 0.001"
      ],
      "id": "coastal-georgia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "restricted-grounds"
      },
      "source": [
        "# Call the train_stochastic() method to update m and c and get error value with lr = 0.001.\n",
        "for num in range(10):\n",
        "    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100)\n",
        "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
        "    y = m * l + c\n",
        "plt.plot(l, tsq, '.k')\n",
        "plt.plot(l, y)\n",
        "plt.xlabel(\"$L$\", fontsize=15)\n",
        "plt.ylabel(\"$T^2$\", fontsize=15)\n",
        "plt.show()"
      ],
      "id": "restricted-grounds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aquatic-recipe"
      },
      "source": [
        "Plot Errors vs Iterations"
      ],
      "id": "aquatic-recipe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "premium-major"
      },
      "source": [
        "ms, cs, errs = [], [], []\n",
        "m, c = 0, 0\n",
        "lr = 0.001\n",
        "\n",
        "# Call the train_stochastic() method to update m and c and get error value with lr = 0.001.\n",
        "for times in range(100):\n",
        "    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100) # We will plot the error values for every 100 iterations\n",
        "    ms.append(m)\n",
        "    cs.append(c)\n",
        "    errs.append(error)\n",
        "plt.plot(range(100), errs);\n",
        "plt.xlabel(\"Iteration\", fontsize=12)\n",
        "plt.ylabel(\"Error\", fontsize=12)\n",
        "plt.show()"
      ],
      "id": "premium-major",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3S-IvdHTJXx"
      },
      "source": [
        "### Minibatch Gradient Descent"
      ],
      "id": "m3S-IvdHTJXx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT6lGuO8TJXy"
      },
      "source": [
        "In Mini-Batch Gradient Descent algorithm, rather than using  the complete data set, in every iteration we use a subset of training examples (called \"batch\") to compute the gradient of the cost function.\n",
        "\n",
        "Common mini-batch sizes range between 50 and 256, but can vary for different applications."
      ],
      "id": "ZT6lGuO8TJXy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pressing-hughes"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Below is a graph that shows the gradient descentâs variants and their direction towards the minimum:\n",
        "![img3](https://miro.medium.com/max/700/1*PV-fcUsNlD9EgTIc61h-Ig.png)"
      ],
      "id": "pressing-hughes"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IWur68qTJXz"
      },
      "source": [
        "`train_one_batch()` : we will be calculating the essential parts of the Gradient Descent method:\n",
        "\n",
        "We assume there are $n$ samples in a batch $B$, for all  $i \\in B$,\n",
        "\n",
        "$y_i = mx_i + c$\n",
        "        \n",
        "$E$ =$\\frac{1}{n}$   $\\sum_{i=1}^n (y - y_i)^2$\n",
        "\n",
        "$\\frac{\\partial E }{\\partial m}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -x_i(y - (mx_i + c))$\n",
        "\n",
        "$\\frac{\\partial E}{\\partial c}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -(y - (mx_i + c))$"
      ],
      "id": "8IWur68qTJXz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJgPCef6TJXz"
      },
      "source": [
        "`train_batches()` : We will be splitting our data into batches."
      ],
      "id": "mJgPCef6TJXz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkRMHxA2TJX0"
      },
      "source": [
        "\"\"\"\n",
        "The function 'train_one_batch()' updates the values of m and c and calculates error.\n",
        "The loss is minimized due to the changed values of m and c.\n",
        "The new values m, c and the minimized loss are returned.\n",
        "\"\"\"\n",
        "def train_one_batch(x, y, m, c, eta):\n",
        "    const = - 2.0/len(y)\n",
        "    ycalc = m * x + c                             # linear regression expression\n",
        "    delta_m = const * sum(x * (y - ycalc))        # dy/dm\n",
        "    delta_c = const * sum(y - ycalc)              # dy/dc\n",
        "    m = m - delta_m * eta                         # gradient descent step to update m\n",
        "    c = c - delta_c * eta                         # gradient descent step to update c\n",
        "    error = sum((y - ycalc)**2)/len(y)            # error\n",
        "    return m, c, error\n",
        "\n",
        "\"\"\"\n",
        "The function below takes a batch_size and loss is calculated w.r.t batches.\n",
        "The batches are created using random index.\n",
        "The m, c and error values are calculated for each batch of data.\n",
        "So, it calls the function 'train_one_batch()' by passing batch_x, batch_y for each batch.\n",
        "\"\"\"\n",
        "def train_batches(x, y, m, c, eta, batch_size):\n",
        "    # Making the batches\n",
        "    random_idx = np.arange(len(y))\n",
        "    np.random.shuffle(random_idx)\n",
        "\n",
        "    # Train each batch\n",
        "    for batch in range(len(y)//batch_size):\n",
        "        batch_idx = random_idx[batch*batch_size:(batch + 1)*batch_size]\n",
        "        batch_x = x[batch_idx]\n",
        "        batch_y = y[batch_idx]\n",
        "        m, c, err = train_one_batch(batch_x, batch_y, m, c, eta)\n",
        "\n",
        "    return m, c, err\n",
        "\n",
        "\"\"\"\n",
        "The function below trains the data for 1000 iterations.\n",
        "The data is traversed in batches, the batch size here is considered to be 10.\n",
        "In each iteration it calls the 'train_batches' function.\n",
        "The 'batch_size' is passed as a parameter to 'train_batches'.\n",
        "\"\"\"\n",
        "def train_minibatch(x, y, m, c, eta, batch_size=10, iterations=1000):\n",
        "    for iteration in range(iterations):\n",
        "        m, c, err = train_batches(x, y, m, c, eta, batch_size)\n",
        "    return m, c, err"
      ],
      "id": "JkRMHxA2TJX0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BLipYesTJX4"
      },
      "source": [
        "#### Train the Minibatch gradient descent"
      ],
      "id": "-BLipYesTJX4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAsrGHdLTJX4"
      },
      "source": [
        "# Initializing m, c\n",
        "m, c = 0, 0\n",
        "# Learning rate\n",
        "lr = 0.001"
      ],
      "id": "oAsrGHdLTJX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y-XhUFqTJYB"
      },
      "source": [
        "# Training for 1000 iterations, plotting after every 100 iterations:\n",
        "\n",
        "# Call the train_minibatch() method to update m and c and get error value with lr = 0.001 and batch_size=90.\n",
        "for num in range(10):\n",
        "    # We will plot the error values for every 100 iterations\n",
        "    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size=90, iterations=100)\n",
        "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
        "    y = m * l + c"
      ],
      "id": "3Y-XhUFqTJYB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHnsERuP2QN4"
      },
      "source": [
        "Visualization of line plot using minibatch gradient descent"
      ],
      "id": "SHnsERuP2QN4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ5vE4AV2FCj"
      },
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(l, tsq, '.k')\n",
        "ax.plot(l, y)\n",
        "plt.xlabel(\"$L$\", fontsize=15)\n",
        "plt.ylabel(\"$T^2$\", fontsize=15)\n",
        "plt.show()"
      ],
      "id": "QZ5vE4AV2FCj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPrP3o7JTJYF"
      },
      "source": [
        "**Ungraded Exercise:** Experiment with other lr values."
      ],
      "id": "bPrP3o7JTJYF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmQ_eNyQTJYG"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "zmQ_eNyQTJYG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stwbu3wYTJYI"
      },
      "source": [
        "**Ungraded Exercise:** Experiment with other batch_size values."
      ],
      "id": "stwbu3wYTJYI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBUcyH_QTJYJ"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "uBUcyH_QTJYJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYt31ypxTE3C"
      },
      "source": [
        "**Comparison of gradient descent variants**\n",
        "\n",
        "![img](https://cdn.iisc.talentsprint.com/CDS/Images/Gradient_descent_variants_comparison.JPG)\n",
        "\n",
        "To know more about gradient descent variants, click [here](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module2/M2_AST_02_Gradient_Descent_Demystifying%20Different%20Variants%20of%20Gradient%20Descent%20Optimization%20Algorithm.pdf)"
      ],
      "id": "IYt31ypxTE3C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "informative-poultry"
      },
      "source": [
        "### Ungraded Question\n",
        "\n",
        "**Question1 :** Determine the gradient of the function $x^2 - 2y^2 - 4y + 6$ at point (0, 0)?\n",
        "\n",
        "\n",
        "**Solution:** âf = 0i â 4j\n",
        "\n",
        "Explanation: At point (0,0), we calculate the gradient at this point as\n",
        "\n",
        "âf/âx = 2x = 2(0) = 0\n",
        "\n",
        "âf/ây = â4y â 4 = â4(0) â 4 = â4\n",
        "\n",
        "which are used to determine the gradient as âf = 0i â 4j"
      ],
      "id": "informative-poultry"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ],
      "id": "VHfHdGCP_n6Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-bZhl8VoIGg"
      },
      "source": [
        "#@title Mark the following statement as True or False: Gradient descent always successfully finds the global minima { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\",\"True\",\"False\"]\n"
      ],
      "id": "f-bZhl8VoIGg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "id": "NMzKSbLIgFzQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "id": "DjcH1VWSFI2l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "id": "4VBk_4VTAxCM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "id": "XH91cL1JWH7m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "id": "z8xLqj7VWIKW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "id": "FzAZHt1zw-Y-",
      "execution_count": null,
      "outputs": []
    }
  ]
}