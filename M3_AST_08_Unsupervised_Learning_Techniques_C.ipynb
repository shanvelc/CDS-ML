{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanvelc/CDS-ML/blob/main/M3_AST_08_Unsupervised_Learning_Techniques_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKnwjn7ApjTM"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 8: Unsupervised Learning Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owyUCpyPpjTO"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVaS2uqqpjTP"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* understand what is Unsupervised Learning\n",
        "* know different types of Unsupervised Learning techniques\n",
        "* perform Clustering (k-means, dbscan)\n",
        "* understand what are Gaussian Mixtures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WG4t4QUTB6Jo"
      },
      "outputs": [],
      "source": [
        "#@title Walkthrough Video\n",
        "from IPython.display import HTML\n",
        "HTML(\"\"\"<video width=\"420\" height=\"240\" controls>\n",
        "<source src=\"https://cdn-exec.ap-south-1.linodeobjects.com/content/Unsupervised_Learning.mp4\">\n",
        "</video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR5JQB6_pjTP"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uazcUKCqpjTP"
      },
      "source": [
        "Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets without human intervention, in contrast to supervised learning where labels are provided along with the data.\n",
        "\n",
        "Some of the unsupervised learning tasks include:\n",
        "\n",
        "* **Clustering:** The goal is to group similar instances together into clusters. It is a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, and more.\n",
        "\n",
        "* **Anomaly detection:** The objective is to learn what “normal” data looks like, and then use that to detect abnormal instances, such as defective items on a production line or a new trend in a time series.\n",
        "\n",
        "* **Density estimation:** This is the task of estimating the probability density function (PDF) of the random process that generated the dataset. Density estimation is commonly used for anomaly detection: instances located in very low-density regions are likely to be anomalies. It is also useful for data analysis and visualization.\n",
        "\n",
        "First we will start with clustering, using k-means and DBSCAN, and then we will discuss Gaussian mixture models and see how they can be used for density estimation, clustering, and anomaly detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "WBPPuGmBlDIN"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_08_Unsupervised_Learning_Techniques_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx pip install yellowbrick\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Images/flowers.jpg\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47LuiF46pjTQ"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ADzXQmpspjTQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, image\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxlkse6qpjTR"
      },
      "source": [
        "### K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOTcysxCpjTR"
      },
      "source": [
        "The k-means algorithm is a simple algorithm capable of clustering very quickly and efficiently, often in just a few iterations.\n",
        "\n",
        "A **cluster** refers to a collection of data points aggregated together because of certain similarities.\n",
        "\n",
        "Here we need to define a target number k, which refers to the number of centroids we need in the dataset. A **centroid** is the imaginary or real location representing the center of the cluster.\n",
        "\n",
        "Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares.\n",
        "In other words, the k-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.\n",
        "\n",
        "Let's see how k-means clusters the species present in Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T4lSCu4gpjTS"
      },
      "outputs": [],
      "source": [
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"]\n",
        "y = iris[\"target\"]\n",
        "X = StandardScaler().fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0q4i_6IQpjTS"
      },
      "outputs": [],
      "source": [
        "# Visualize the data using two features\n",
        "plt.scatter(X[:,2], X[:,3])\n",
        "plt.xlabel('petal length (cm)')\n",
        "plt.ylabel('petal width (cm)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D1OQvIohpjTT"
      },
      "outputs": [],
      "source": [
        "# Perform k-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=1)\n",
        "y_predict = kmeans.fit_predict(X)\n",
        "plt.scatter(X[:,2], X[:,3], c=y_predict, cmap = 'summer')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXqLUXc1pjTU"
      },
      "source": [
        "From the above plot, we can see that only by specifying the number of clusters k-means identifies them within the dataset.\n",
        "\n",
        "Now let's see how the k-means algorithm works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JtDbb51pjTU"
      },
      "source": [
        "#### Working of k-means algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EYiGtnnpjTU"
      },
      "source": [
        "It starts by placing the centroids randomly (e.g., by picking k instances at random and using their locations as centroids). Then label the instances and update the centroids, then again label the instances and update the centroids, and so on until the centroids stop moving. The algorithm converges in a finite number of steps.\n",
        "\n",
        "In order to update the label of instances, k-means computes the distance of each instance from every cluster and assigns the one which is closest to them. Also, each centroid  is updated to the mean of all instances assigned to that cluster as shown in the figure below.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Kmeans_cluster_update.JPG\" width=650/>\n",
        "</center>\n",
        "\n",
        "The algorithm halts creating and optimizing clusters when either:\n",
        "\n",
        "* The centroids have stabilized — there is no change in their values because the clustering has been successful.\n",
        "* The defined number of iterations has been achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWFwadcdpjTV"
      },
      "source": [
        "#### Centroid initialization methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh5vW_f0pjTV"
      },
      "source": [
        "If we happen to know approximately where the centroids should be, then we can set the `init` hyperparameter to a NumPy array containing the list of centroids, and set number of iteration (`n_init`) to 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JNvswJRkpjTV"
      },
      "outputs": [],
      "source": [
        "# Perform k-means clustering with specified centroids\n",
        "good_init = np.array([[-1, -1], [0, 0], [1.5, 1.5]])\n",
        "kmeans = KMeans(n_clusters = 3, init = good_init, n_init = 1, random_state = 1)\n",
        "# Make prediction using only two features, as centroids are specified for only two features\n",
        "y_predict = kmeans.fit_predict(X[:,2:])\n",
        "\n",
        "# Plot initial centroids\n",
        "plt.scatter(x = [-1, 0, 1.5], y = [-1, 0, 1.5], c = 'magenta', s = 100, marker= \"^\")\n",
        "# Plot data points\n",
        "plt.scatter(X[:,2], X[:,3], c = y_predict, cmap = 'summer')\n",
        "# Plot centroids after iteration\n",
        "plt.scatter(x = kmeans.cluster_centers_[:,0], y = kmeans.cluster_centers_[:,1], c = 'r', marker=\"*\", s = 200)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UNJaS7ypjTW"
      },
      "source": [
        "From the above plot, we can see the initial centroids in magenta triangle and centroids after k-means in red star. So by specifying the centroids initially we can reduce the number of iterations required. By default it uses 10, which means that the whole algorithm runs 10 times when we call fit(), and Scikit-Learn keeps the best solution.\n",
        "\n",
        "It knows which solution is the best by using a performance metric called the model’s **inertia**, which is the mean squared distance between each instance and its closest centroid. The `KMeans` class runs the algorithm `n_init` times and keeps the model with the **lowest inertia**.\n",
        "\n",
        "We can access a model's inertia via the `inertia_` instance variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PsbSc7IOpjTW"
      },
      "outputs": [],
      "source": [
        "# Model's inertia\n",
        "kmeans.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG7WvRvQpjTW"
      },
      "source": [
        "An important improvement to the k-means algorithm, **K-Means++**, was proposed in\n",
        "a 2006 paper by David Arthur and Sergei Vassilvitskii. They introduced a smarter\n",
        "initialization step that tends to select centroids that are distant from one another, and this improvement makes the k-means algorithm much less likely to converge to a suboptimal solution.\n",
        "\n",
        "The K-Means++ initialization algorithm is as follow:\n",
        "\n",
        "1. Take one centroid $c^{(1)}$, chosen uniformly at random from the dataset.\n",
        "\n",
        "2. Take a new centroid $c^{(i)}$, choosing an instance $x^{(i)}$ with probability $D(x^{(i)})^2/ \\sum_{j=1}^{m}D(x^{(j)})^2$, where $D(x^{(i)})$ is the distance between the instance $x^{(i)}$ and the closest centroid that was already chosen. This probability distribution ensures that instances farther away from already chosen centroids are much more likely be selected as centroids.\n",
        "\n",
        "3. Repeat the previous step until all k centroids have been chosen.\n",
        "\n",
        "The `KMeans` class uses this initialization method by default. If we want to force it to use the original method (i.e., picking k instances randomly to define the initial centroids), then we can set the `init=\"random\"` but we will rarely need to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMTFWAcbpjTW"
      },
      "source": [
        "Till now we are specifying the number of clusters because we know there are three species in Iris dataset. But what to do when we don't know how many clusters the dataset contains or it is hard to identify even with visualization.\n",
        "\n",
        "Let's see how we find optimal number of clusters then."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9LtQZJ7pjTX"
      },
      "source": [
        "#### Finding the optimal number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAAJotM0pjTX"
      },
      "source": [
        "The inertia is not a good performance metric when trying to choose $k$ because it keeps getting lower as we increase $k$. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be.\n",
        "\n",
        "Let’s plot the inertia as a function of $k$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tD-t32vLpjTX"
      },
      "outputs": [],
      "source": [
        "# Plot inertia by varying number of clusters\n",
        "clusters = np.arange(1,10)\n",
        "inertia = []\n",
        "for c in clusters:\n",
        "    kmeans = KMeans(n_clusters = c, random_state=1)\n",
        "    kmeans.fit(X, y)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "plt.plot(clusters, inertia, marker= '.')\n",
        "plt.arrow(5, 400, -2, -252)\n",
        "plt.text(5, 410, \"Elbow\", fontdict={'size': 12})\n",
        "plt.title('Inertia Plot')\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p28qpRzZpjTX"
      },
      "source": [
        "From the above plot we can see, the inertia drops very quickly as we increase $k$ up to 3, but then it decreases much more slowly as we keep increasing $k$. This curve has roughly the shape of an arm, and there is an **“elbow”** at $k = 3$. So, if we did not know better, $3$ would be a good choice.\n",
        "\n",
        "This technique for choosing the best value for the number of clusters is rather coarse. A **more precise approach** but also more computationally expensive is to use the **silhouette score**, which is the mean silhouette coefficient over all the instances.\n",
        "\n",
        "An instance’s silhouette coefficient is given by $$Sil(x_1) = \\frac{(b – a)}{max(a, b)}$$ where,\n",
        "\n",
        "$a$ is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance), and\n",
        "\n",
        "$b$ is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next closest cluster, defined as the one that minimizes $b$, excluding the instance’s own cluster) as shown in the figure below.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Silhouette_coefficient.png\" width= 500 px/>\n",
        "</center>\n",
        "\n",
        "The silhouette coefficient can vary between $–1$ and $+1$ as follows:\n",
        "\n",
        "* close to $+1$ means that the instance is well inside its own cluster and far from other clusters,\n",
        "* close to $0$ means that it is close to a cluster boundary, and  \n",
        "* close to $–1$ means that the instance may have been assigned to the wrong cluster.\n",
        "\n",
        "To compute the silhouette score, we can use Scikit-Learn’s silhouette_score() function, giving it all the instances in the dataset and the labels they were assigned:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PTGIyWrrpjTX"
      },
      "outputs": [],
      "source": [
        "# Plot Silhouette score plot\n",
        "\n",
        "clusters = np.arange(2,10)\n",
        "sil_score = []\n",
        "for c in clusters:\n",
        "    kmeans = KMeans(n_clusters = c, random_state=1)\n",
        "    kmeans.fit(X, y)\n",
        "    sil_score.append(silhouette_score(X, kmeans.labels_))\n",
        "plt.plot(clusters, sil_score, marker= '.')\n",
        "plt.title('Silhouette score plot')\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Silhouette score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa36f_E_pjTY"
      },
      "source": [
        "As we can see, the above visualization is much richer than the previous one: although it shows that $k = 2$ is a good choice, it also underlines the fact that $k = 3$ is quite good as well and much better than $k = 4$ or $5$. This was not visible when comparing inertias.\n",
        "\n",
        "An even more informative visualization is obtained when we plot every instance’s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a **silhouette diagram**.\n",
        "\n",
        "Diagram contains:\n",
        "* one knife shape per cluster\n",
        "* the shape’s height indicates the number of instances the cluster contains, and\n",
        "* its width represents the sorted silhouette coefficients of the instances in the cluster (wider is better).\n",
        "\n",
        "The dashed line indicates the mean silhouette coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWWekB-PpjTY"
      },
      "outputs": [],
      "source": [
        "# Plot Silhouette Diagram\n",
        "\n",
        "clusters = [2, 3, 4]\n",
        "for c in clusters:\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    # Instantiate the clustering model and visualizer\n",
        "    kmeans = KMeans(c, random_state=1)\n",
        "    # Instantiate SilhouetteVisualizer()\n",
        "    visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
        "    # Fit the data to the visualizer\n",
        "    visualizer.fit(X);\n",
        "    plt.title(\"k={}\".format(c))\n",
        "    plt.xlabel(\"Silhoutte score\")\n",
        "    plt.ylabel(\"Number of Instances\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrNGXtD0pjTY"
      },
      "source": [
        "In the above plots, the vertical dashed lines represent the silhouette score for each number of clusters. When most of the instances in a cluster have a lower coefficient than this score (ending to the left of it), then the cluster is rather bad since this means its instances are much too close to other clusters.\n",
        "\n",
        "We can see that when k = 2 or k = 3, the clusters look pretty good: most instances extend beyond the dashed line, to the right and closer to 1.0. When $k = 2$, the cluster at index $0$ is rather big. When $k = 3$, all clusters have similar sizes. So, even though the overall silhouette score from $k = 2$ is slightly greater than for $k = 3$, it seems like a good choice to use $k = 3$ to get clusters of similar sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w_PODwdpjTY"
      },
      "source": [
        "Now let’s look at a few ways we can benefit from k-means clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PXaSSJvpjTZ"
      },
      "source": [
        "#### Using Clustering for Image Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8KjaXRipjTZ"
      },
      "source": [
        "Image segmentation is the task of partitioning an image into multiple segments.\n",
        "\n",
        "Here, we are going to do **color segmentation**. We will simply assign pixels to the same segment if they have a similar color. In some applications, this may be sufficient. For example, if we want to analyze satellite images to measure how much total forest area there is in a region, color segmentation may be just fine.\n",
        "\n",
        "Let's performs color segmentation on the flowers image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvcTKaOipjTZ"
      },
      "outputs": [],
      "source": [
        "# Import flowers image\n",
        "img = image.imread(\"flowers.jpg\")\n",
        "# Noremalize its pixel value\n",
        "img = img / 255\n",
        "plt.imshow(img)\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRXVwm3UpjTZ"
      },
      "source": [
        "The image is represented as a 3D array. The first dimension’s size is the height; the second is the width; and the third is the number of color channels, in this case red, green, and blue (RGB).\n",
        "\n",
        "For segmentation,\n",
        "\n",
        "* First, it identify a color cluster for all shades of that color.\n",
        "\n",
        "* Next, for each color, it looks for the mean color of the pixel’s color cluster. For example, all shades of green may be replaced with the same green color.\n",
        "\n",
        "* Finally, it reshapes this long list of colors to get the same shape as the original image.\n",
        "\n",
        "The following code shows clustering using k-means:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApCV6GJWpjTZ"
      },
      "outputs": [],
      "source": [
        "# Color segmentation for different number of clusters\n",
        "fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(12,10))\n",
        "\n",
        "X_ = img.reshape(-1, 3)\n",
        "clusters = [2, 3, 4, 5]\n",
        "for axi, c in zip([ax4, ax3, ax2, ax1], clusters):\n",
        "    kmeans = KMeans(n_clusters = c)\n",
        "    kmeans.fit(X_)\n",
        "    # identify a color cluster\n",
        "    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
        "    segmented_img = segmented_img.reshape(img.shape)\n",
        "    axi.imshow(segmented_img)\n",
        "    axi.set_title(\"k = {}\".format(c), size=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi7Bvn2dpjTa"
      },
      "source": [
        "When we use fewer than five clusters, notice that the bud's shinny color fails to get a cluster of its own: it gets merged with colors from the environment. This is because k-means prefer clusters of similar sizes. The buds are small, much smaller than the rest of the image, so even though its color is flashy, k-means fails to dedicate a cluster to it.\n",
        "\n",
        "Now let’s look at another application of clustering: i.e, in Semi-Supervised Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjN3BenbpjTa"
      },
      "source": [
        "#### Using Clustering for Semi-Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeAGi7JRpjTa"
      },
      "source": [
        "Another use case for clustering is in semi-supervised learning when we have plenty of unlabeled instances and very few labeled instances. Let’s train a Logistic Regression model on a sample of 50 labeled instances from the digits dataset which is a simple MNIST-like dataset containing 1,797 grayscale 8 × 8 images representing the digits 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_77prLINpjTa"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, test_size = 0.25, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiwToHC-pjTa"
      },
      "outputs": [],
      "source": [
        "# Logistic regression on 50 random labeled instances\n",
        "n_labeled = 50\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\n",
        "log_reg.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8hK4eDhpjTb"
      },
      "source": [
        "The accuracy is just 81.1%. It should come as no surprise that this is much lower than if we trained the model on the full training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZQut4VvpjTb"
      },
      "outputs": [],
      "source": [
        "# Logistic regression on full training set\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "log_reg.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCTUrF2hpjTb"
      },
      "source": [
        "Let’s see how we can do better.\n",
        "\n",
        "First, let’s cluster the training set into 50 clusters. Then for each cluster, let’s find the image closest to the centroid. We will call these images the representative images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usH-VsDcpjTc"
      },
      "outputs": [],
      "source": [
        "# Cluster the training instances\n",
        "\n",
        "k = 50\n",
        "kmeans = KMeans(n_clusters=k, random_state = 123)\n",
        "# compute distances from each cluster\n",
        "X_digits_dist = kmeans.fit_transform(X_train)\n",
        "# get indices of images closest to their cluster\n",
        "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
        "# get the images corresponds to indices\n",
        "X_representative_digits = X_train[representative_digit_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVjoJV7_pjTc"
      },
      "source": [
        "Let’s look at each image and manually label it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlurrZs9pjTc"
      },
      "outputs": [],
      "source": [
        "# Visualize representative images\n",
        "fig, ax = plt.subplots(10,5)\n",
        "for axi, i in zip(ax.ravel(), np.arange(0,50)):\n",
        "    axi.imshow(X_representative_digits[i].reshape(8,8), cmap='Greys');\n",
        "    axi.grid(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8ys3QQ1pjTc"
      },
      "outputs": [],
      "source": [
        "# Representative image labels\n",
        "y_representative_digits = y_train[representative_digit_idx]\n",
        "y_representative_digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3soHkiEipjTc"
      },
      "source": [
        "Now we have a dataset with just 50 labeled instances, but instead of being random instances, each of them is a representative image of its cluster. Let’s see if the performance is any better:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWdAiYU1pjTc"
      },
      "outputs": [],
      "source": [
        "# Logistic regression on 50 representative labeled instances\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_representative_digits, y_representative_digits)\n",
        "log_reg.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCMu2YJMpjTd"
      },
      "source": [
        "From the above results, we can see that accuracy jumped from 81.1% to 90%, although we are still only training the model on 50 instances.\n",
        "\n",
        "Since it is often costly and painful to label instances, especially when it has to be done manually, it is a good idea to label representative instances rather than just random instances.\n",
        "\n",
        "Also, we can go one step further: that is if we propagate the labels to all the other instances in the same cluster. This method is called **label propagation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoVzQoiqpjTd"
      },
      "outputs": [],
      "source": [
        "# Propagate labels\n",
        "y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
        "for i in range(k):\n",
        "    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjy3NUdypjTd"
      },
      "source": [
        "Now let’s train the model again and look at its performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTaGcc7GpjTd"
      },
      "outputs": [],
      "source": [
        "# Model after label propagation\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train_propagated)\n",
        "log_reg.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN3uOHW9pjTe"
      },
      "source": [
        "From the above result, we can see that we got a reasonable accuracy boost from 90% to 91.5%.\n",
        "\n",
        "Here we propagated each representative instance’s label to all the instances in the same cluster, including the instances located close to the cluster boundaries, which are more likely to be mislabeled.\n",
        "\n",
        "Let’s see what happens if we only propagate the labels to the 30% of the instances that are closest to the centroids:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPaV-Y3GpjTe"
      },
      "outputs": [],
      "source": [
        "# Propagate only to 30% closest instances\n",
        "\n",
        "percentile_closest = 30\n",
        "# distance to closest cluster for each instance\n",
        "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
        "for i in range(k):\n",
        "    in_cluster = (kmeans.labels_ == i)\n",
        "    cluster_dist = X_cluster_dist[in_cluster]\n",
        "    # threshold distance\n",
        "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
        "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
        "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
        "\n",
        "partially_propagated = (X_cluster_dist != -1)\n",
        "X_train_partially_propagated = X_train[partially_propagated]\n",
        "y_train_partially_propagated = y_train_propagated[partially_propagated]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkqrFbgNfSbS"
      },
      "source": [
        "Now let’s train the model again on this partially propagated dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A9LikgtpjTe"
      },
      "outputs": [],
      "source": [
        "# Model after label propagation to 30% closest instances\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
        "log_reg.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HbHfkYGpjTe"
      },
      "source": [
        "From the above results, we can see that with just 50 labeled instances (only 5 examples per class on average!), we got 91.7% accuracy, which is pretty close to the performance of Logistic Regression on the fully labeled digits dataset (which was 96%).\n",
        "\n",
        "This good performance is due to the fact that the propagated labels are actually pretty good, their accuracy is very close to 99%, as shown in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2IjbB5upjTe"
      },
      "outputs": [],
      "source": [
        "# Accuracy of propagated labels w.r.t actual labels\n",
        "np.mean(y_train_partially_propagated == y_train[partially_propagated])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGOC2mDHpjTf"
      },
      "source": [
        "Now, let’s take a look at DBSCAN, another popular clustering algorithm that illustrates a very different approach based on local density estimation. This approach allows the algorithm to identify clusters of arbitrary shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAW8j0ippjTf"
      },
      "source": [
        "### DBSCAN (Density-based spatial clustering of applications with noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbOp7XqepjTf"
      },
      "source": [
        "This algorithm defines clusters as continuous regions of high density. Here is how it works:\n",
        "\n",
        "* For each instance, the algorithm counts how many instances are located within a small distance $ε$ (epsilon) from it. This region is called the instance’s $ε$-neighborhood.\n",
        "\n",
        "* If an instance has at least `min_samples` instances in its $ε$-neighborhood (including itself), then it is considered a core instance. In other words, core instances are those that are located in dense regions.\n",
        "\n",
        "* All instances in the neighborhood of a core instance belong to the same cluster. This neighborhood may include other core instances; therefore, a long sequence of neighboring core instances forms a single cluster.\n",
        "\n",
        "* Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly.\n",
        "\n",
        "This algorithm works well if all the clusters are dense enough and if they are well separated by low-density regions.\n",
        "\n",
        "Let’s test it on the `moons` dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnbFxWrCpjTf"
      },
      "outputs": [],
      "source": [
        "# Visualize data\n",
        "X_moons, y_moons = datasets.make_moons(n_samples=1000, noise=0.07, random_state=1)\n",
        "plt.scatter(X_moons[:,0], X_moons[:,1], cmap= \"autumn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMyltkmwpjTf"
      },
      "source": [
        "The labels of all the instances can be access via the `labels_` instance variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V-FXjDspjTf"
      },
      "outputs": [],
      "source": [
        "# Perform DBSCAN on data\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
        "dbscan.fit(X_moons)\n",
        "print(\"Unique clusters in data: \", np.unique(dbscan.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFfFy86XpjTf"
      },
      "source": [
        "Notice that some instances have a cluster index equal to $–1$, which means that they are considered as **anomalies** by the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IXlfBKWpjTg"
      },
      "outputs": [],
      "source": [
        "# Visualize data with DBSCAN labels\n",
        "plt.scatter(X_moons[:,0], X_moons[:,1], c = dbscan.labels_, cmap= \"autumn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEIS_We6pjTg"
      },
      "source": [
        "From the above plot, as we can see, it identified quite a lot of anomalies, plus two different clusters, which looks perfect.\n",
        "\n",
        "The indices of the core instances are available in the `core_sample_indices_` instance variable, and the core instances themselves are available in the `components_` instance variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU1gbj1PpjTg"
      },
      "outputs": [],
      "source": [
        "# Number of core instances their indices and position\n",
        "num_of_core_instns = len(dbscan.core_sample_indices_)\n",
        "print(\"Number of core instances: \", num_of_core_instns)\n",
        "\n",
        "core_instns_indices = dbscan.core_sample_indices_\n",
        "print(\"First 10 core instances indices: \", core_instns_indices[0:10])\n",
        "\n",
        "core_instns = dbscan.components_\n",
        "print(\"First 10 core instances: \\n\", core_instns[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWELl1XXpjTg"
      },
      "source": [
        "DBSCAN class does not have a `predict()` method, although it has a `fit_predict()` method. In other words, it cannot predict which cluster a new instance belongs to.\n",
        "\n",
        "This implementation decision was made because different classification algorithms can be better for different tasks, so the authors decided to let the user choose which one to use. For example, let’s train a `KNeighborsClassifier`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai-ZBlU5pjTg"
      },
      "outputs": [],
      "source": [
        "# Training KNeighborsClassifier on core instances\n",
        "knn = KNeighborsClassifier(n_neighbors = 50)\n",
        "X_core = core_instns\n",
        "y_core = dbscan.labels_[core_instns_indices]\n",
        "knn.fit(X_core, y_core)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c9q8vjZpjTg"
      },
      "source": [
        "Now, given a few new instances, we can predict which cluster they most likely belong to and even estimate a probability for each cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVs4eWI7pjTh"
      },
      "outputs": [],
      "source": [
        "# Prediction on new instances\n",
        "X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
        "pred = knn.predict(X_new)\n",
        "print(\"Prediction: \", pred)\n",
        "\n",
        "pred_prob = knn.predict_proba(X_new)\n",
        "print(\"Prediction probability: \\n\", pred_prob)\n",
        "\n",
        "# Visualize prediction\n",
        "plt.scatter(X_core[:,0], X_core[:,1], c = y_core, cmap= \"autumn\", marker = '.')\n",
        "plt.scatter(x = [-0.5, 0, 1, 2], y = [0, 0.5, -0.1, 1], c = pred, cmap='autumn', s=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXTOUcgJpjTh"
      },
      "source": [
        "Notice that since there is no anomaly in the training set (only core instances used), the classifier always chooses a cluster, even when that cluster is far away.\n",
        "\n",
        "It is fairly straightforward to introduce a maximum distance, in which case the two instances that are far away from both clusters are classified as anomalies.\n",
        "\n",
        "To do this, we have the `kneighbors()` method of the `KNeighborsClassifier`. Given a set of instances, it returns the distances and the indices of the k nearest neighbors in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWd2fma7pjTh"
      },
      "outputs": [],
      "source": [
        "# Distances and indices of k nearest neighbors\n",
        "y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\n",
        "y_pred = y_core[y_pred_idx]\n",
        "# Introduce a maximum distance\n",
        "y_pred[y_dist > 0.1] = -1\n",
        "pred1 = y_pred.ravel()\n",
        "pred1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CetewSDIpjTh"
      },
      "outputs": [],
      "source": [
        "# Visualize prediction\n",
        "plt.scatter(X_core[:,0], X_core[:,1], c = y_core, cmap= \"autumn\", marker = '.')\n",
        "plt.scatter(x = [-0.5, 0, 1, 2], y = [0, 0.5, -0.1, 1], c = ['k', 'r', 'y', 'k'], s=100 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wn5hmVSpjTh"
      },
      "source": [
        "From the above plot, it can be seen that two anomalies are separated out.\n",
        "\n",
        "**Note that** we only trained the classifier (KNN) on the core instances, but we could also have chosen to train it on all the instances, or all but the anomalies: this choice depends on the final task the model is building for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkzxkBtmpjTh"
      },
      "source": [
        "Now let’s dive into Gaussian mixture models, which can be used for density estimation, clustering, and anomaly detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th8bH_fdpjTi"
      },
      "source": [
        "### Gaussian Mixtures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRG2801XpjTi"
      },
      "source": [
        "A **Gaussian mixture model (GMM)** is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape, size, density, and orientation.\n",
        "\n",
        "There are several GMM variants. In the simplest variant, implemented in the `GaussianMixture` class, we must know in advance the number $k$ of Gaussian distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd6s6-8EpjTi"
      },
      "source": [
        "Let's see the performance of GMM model on Iris dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCRHRw1EpjTi"
      },
      "outputs": [],
      "source": [
        "# Gaussian Mixture model\n",
        "gm = GaussianMixture(n_components=3, n_init=10)\n",
        "gm.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIHOhH8fpjTi"
      },
      "outputs": [],
      "source": [
        "# Make prediction\n",
        "predict = gm.predict(X)\n",
        "\n",
        "# Visualize prediction labels\n",
        "plt.scatter(X[:,2], X[:,3], c = predict, cmap= \"autumn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBoPUxRrpjTi"
      },
      "source": [
        "From the above plot, we can see that using GMM algorithm the labels are classified quite well.\n",
        "\n",
        "Let’s look at the parameters that the algorithm estimated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP_eIuHmpjTj"
      },
      "outputs": [],
      "source": [
        "# Estimated weights, means, covariance matrices of clusters\n",
        "print(\"Weights: \", gm.weights_)\n",
        "print(\"Means: \", gm.means_)\n",
        "print(\"Covariance matrices: \\n\", gm.covariances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEF3Y9NqpjTj"
      },
      "source": [
        "From the above results, the weights that were used to generate the data were $0.36, 0.33,$ and $0.30$.\n",
        "\n",
        "Now, let's visualize the estimated means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7PQbswjpjTj"
      },
      "outputs": [],
      "source": [
        "# Visualize means\n",
        "plt.scatter(X[:,2], X[:,3], c = predict, cmap= \"autumn\")\n",
        "plt.scatter(gm.means_[:,2], gm.means_[:,3], marker=\"*\", s=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0WAT8wAmpV7"
      },
      "source": [
        "We can check whether or not the algorithm converged and how many iterations it\n",
        "took:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHLrEp4hmsE_"
      },
      "outputs": [],
      "source": [
        "# Algorithm convergence and number of iterations\n",
        "print(\"Algorithm converged: \", gm.converged_)\n",
        "print(\"Number of EM iterations used: \", gm.n_iter_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FODX7jD9FcOu"
      },
      "source": [
        "Now let's see how GMM works for outlier detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmjLrGzCpjTk"
      },
      "source": [
        "#### Anomaly Detection Using Gaussian Mixtures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bhrY2n9pjTl"
      },
      "source": [
        "Using a GMM for anomaly detection is quite simple: any instance located in a low-density region can be considered an anomaly. We must define what density threshold we want to use.\n",
        "\n",
        "For example, in a manufacturing company that tries to detect defective products, the ratio of defective products is usually well known. Say it is equal to 4%. We then set the density threshold to be the value that results in having 4% of the instances located in areas below that threshold density.\n",
        "\n",
        "Also,\n",
        "\n",
        "* If we notice that we get too many false positives (i.e., perfectly good products that are flagged as defective), we can lower the threshold.\n",
        "\n",
        "* Conversely, if we have too many false negatives (i.e., defective products that the system does not flag as defective), we can increase the threshold. This is the usual precision/recall trade-off.\n",
        "\n",
        "Here is how we would identify the outliers using the fourth percentile lowest density as the threshold (i.e., approximately 4% of the instances will be flagged as anomalies) in Iris dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryV4IUxYpjTl"
      },
      "outputs": [],
      "source": [
        "# Density threshold\n",
        "densities = gm.score_samples(X)\n",
        "density_threshold = np.percentile(densities, 4)\n",
        "anomalies = X[densities < density_threshold]\n",
        "anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJkt-LtfpjTl"
      },
      "outputs": [],
      "source": [
        "# Visualize anomalies\n",
        "plt.scatter(X[:,2], X[:,3], c = predict, cmap= \"autumn\")\n",
        "plt.scatter(anomalies[:,2], anomalies[:,3], c = 'k', marker=\"*\", s=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43SuX_FHpjTl"
      },
      "source": [
        "In the above plot, as the actual training set is in 4D, we might find some ambiguity in visualizing anomalies in 2D.\n",
        "\n",
        "Till now we have seen that just like k-means, the Gaussian Mixture algorithm requires us to specify the number of clusters. So, let's see how we can select it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4giylEFpjTl"
      },
      "source": [
        "#### Selecting the Number of Clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D-WuPXopjTl"
      },
      "source": [
        "With k-means, we could use the inertia or the silhouette score to select the appropriate number of clusters. But with Gaussian mixtures, it is not possible to use these metrics because they are not reliable when the clusters are not spherical or have different sizes.\n",
        "\n",
        "Instead, we can try to find the model that minimizes a theoretical information criterion, such as the **Bayesian information criterion (BIC)** or the **Akaike information criterion (AIC)**, defined as:\n",
        "\n",
        "$$BIC = (log(m))p − 2log(\\hat{L})$$\n",
        "\n",
        "$$AIC = 2p − 2 log(\\hat{L})$$\n",
        "\n",
        "where,\n",
        "\n",
        "$m$ is the number of instances\n",
        "\n",
        "$p$ is the number of parameters learned by the model.\n",
        "\n",
        "$L$ is the maximized value of the likelihood function of the model.\n",
        "\n",
        "Both the BIC and the AIC penalize models that have more parameters to learn (e.g., more clusters) and reward models that fit the data well. They often end up selecting the same model. When they differ, the model selected by the BIC tends to be simpler (fewer parameters) than the one selected by the AIC, but tends to not fit the data quite as well (this is especially true for larger datasets).\n",
        "\n",
        "To compute the BIC and AIC, we have the `bic()` and `aic()` methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0Qi06ePpjTm"
      },
      "outputs": [],
      "source": [
        "# Plot BIC and AIC by varying number of clusters\n",
        "clusters = np.arange(1,5)\n",
        "bic = []\n",
        "aic = []\n",
        "for c in clusters:\n",
        "    gmm = GaussianMixture(n_components=c, n_init=10, random_state=123)\n",
        "    gmm.fit(X)\n",
        "    bic.append(gmm.bic(X))\n",
        "    aic.append(gmm.aic(X))\n",
        "\n",
        "plt.plot(clusters, bic, marker= '.', c='b', ls='-')\n",
        "plt.plot(clusters, aic, marker= '.', c='r', ls='--')\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Information Criterion\")\n",
        "plt.legend([\"BIC\", \"AIC\"], prop=dict(size=14), frameon= True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9oxjsu0pjTm"
      },
      "source": [
        "From the above plot we can see, both the BIC and the AIC are considerably lower when $k=3$, so it is most likely the best choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZn1Zeo-pjTm"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6liu0MDpjTm"
      },
      "source": [
        "1. How would you define clustering? Can you name a few clustering algorithms?\n",
        "\n",
        "  In Machine Learning, clustering is the unsupervised task of grouping similar\n",
        "instances together. The notion of similarity depends on the task at hand: for\n",
        "example, in some cases two nearby instances will be considered similar, while in\n",
        "others similar instances may be far apart as long as they belong to the same\n",
        "densely packed group. Popular clustering algorithms include k-means,\n",
        "DBSCAN, agglomerative clustering, BIRCH, Mean-Shift, affinity propagation,\n",
        "and spectral clustering.\n",
        "\n",
        "2. What are some of the main applications of clustering algorithms?\n",
        "\n",
        "  The main applications of clustering algorithms include data analysis, customer\n",
        "segmentation, recommender systems, search engines, image segmentation, semisupervised learning, dimensionality reduction, anomaly detection, and novelty\n",
        "detection.\n",
        "\n",
        "3. Describe two techniques to select the right number of clusters when using k-means.\n",
        "\n",
        "  The elbow rule is a simple technique to select the number of clusters when using\n",
        "k-means: just plot the inertia (the mean squared distance from each instance to\n",
        "its nearest centroid) as a function of the number of clusters, and find the point in the curve where the inertia stops dropping fast (the “elbow”). This is generally close to the optimal number of clusters. Another approach is to plot the silhouette score as a function of the number of clusters. There will often be a peak, and the optimal number of clusters is generally nearby. The silhouette score is the mean silhouette coefficient over all instances. This coefficient varies from +1 for instances that are well inside their cluster and far from other clusters, to –1 for instances that are very close to another cluster. You may also plot the silhouette diagrams and perform a more thorough analysis.\n",
        "\n",
        "4. What is label propagation? Why would you implement it, and how?\n",
        "\n",
        "  Labeling a dataset is costly and time-consuming. Therefore, it is common to have plenty of unlabeled instances, but few labeled instances. Label propagation is a technique that consists in copying some (or all) of the labels from the labeled instances to similar unlabeled instances. This can greatly extend the number of labeled instances, and thereby allow a supervised algorithm to reach better performance (this is a form of semi-supervised learning). One approach is to use a clustering algorithm such as k-means on all the instances, then for each cluster find the most common label or the label of the most representative instance (i.e., the one closest to the centroid) and propagate it to the unlabeled instances in the same cluster.\n",
        "\n",
        "5. Can you name two clustering algorithms that can scale to large datasets? And\n",
        "two that look for regions of high density?\n",
        "\n",
        "  k-means and BIRCH scale well to large datasets. DBSCAN and Mean-Shift look\n",
        "for regions of high density.\n",
        "\n",
        "6. Can you think of a use case where active learning would be useful? How would\n",
        "you implement it?\n",
        "\n",
        "  Active learning is useful whenever you have plenty of unlabeled instances but\n",
        "labeling is costly. In this case (which is very common), rather than randomly\n",
        "selecting instances to label, it is often preferable to perform active learning,\n",
        "where human experts interact with the learning algorithm, providing labels for specific instances when the algorithm requests them. A common approach is\n",
        "uncertainty sampling.\n",
        "\n",
        "7. What is the difference between anomaly detection and novelty detection?\n",
        "\n",
        "  Many people use the terms anomaly detection and novelty detection interchangeably, but they are not exactly the same. In anomaly detection, the algorithm is trained on a dataset that may contain outliers, and the goal is typically to identify these outliers (within the training set), as well as outliers among new instances. In novelty detection, the algorithm is trained on a dataset that is presumed to be “clean,” and the objective is to detect novelties strictly among new instances. Some algorithms work best for anomaly detection (e.g., Isolation Forest), while others are better suited for novelty detection (e.g., one-class SVM).\n",
        "\n",
        "8. What is a Gaussian mixture? What tasks can you use it for?\n",
        "\n",
        "  A Gaussian mixture model (GMM) is a probabilistic model that assumes that the\n",
        "instances were generated from a mixture of several Gaussian distributions whose\n",
        "parameters are unknown. In other words, the assumption is that the data is grouped into a finite number of clusters, each with an ellipsoidal shape (but the clusters may have different ellipsoidal shapes, sizes, orientations, and densities), and we don’t know which cluster each instance belongs to. This model is useful for density estimation, clustering, and anomaly detection.\n",
        "\n",
        "9. Can you name two techniques to find the right number of clusters when using a\n",
        "Gaussian mixture model?\n",
        "\n",
        "  One way to find the right number of clusters when using a Gaussian mixture\n",
        "model is to plot the Bayesian information criterion (BIC) or the Akaike information criterion (AIC) as a function of the number of clusters, then choose the\n",
        "number of clusters that minimizes the BIC or AIC. Another technique is to use a\n",
        "Bayesian Gaussian mixture model, which automatically selects the number of\n",
        "clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title In which of the following algorithms cluster parameter initialization is required at beginning: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"k-means and dbscan\", \"dbscan and gaussian mixtures\", \"k-means and gaussian mixtures\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}