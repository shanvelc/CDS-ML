{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanvelc/CDS-ML/blob/main/M3_AST_06_Boosting_Stacking_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG0lhaPsH506"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 6: Boosting and Stacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeP1PAXf8jYD"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfvGbXGGICEg"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "- understand various boosting methods\n",
        "- implement AdaBoost, Gradient Boosting and Extreme Gradient Boosting\n",
        "- implement stacking algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Walkthrough Video\n",
        "from IPython.display import HTML\n",
        "HTML(\"\"\"<video width=\"420\" height=\"240\" controls>\n",
        "<source src=\"https://cdn.chn.talentsprint.com/content/Boosting_and_Stacking_walkthrough.mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jvB3xnkbMCgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcici309IMKU"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7Cphim7IdBb"
      },
      "source": [
        "Boosting is a general ensemble method that creates a strong model from a number of weak learners.\n",
        "\n",
        "This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n",
        "\n",
        "There  are  many  boosting  methods  available,  but  by  far  the  most  popular  are AdaBoost (short  for  Adaptive  Boosting)  and  Gradient  Boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLgYBjizJeMz"
      },
      "source": [
        "#### AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAZJCwi3Jg3a"
      },
      "source": [
        "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors  focusing  more  and  more  on  the  hard  cases.  This  is  the  technique  used  by  AdaBoost. For example, to build an AdaBoost classifier, a first base classifier (such as a Decision Tree) is trained and used to make predictions on the training set. The relative weight of  misclassified  training  instances  is  then  increased.  A  second  classifier  is  trained using the updated weights and again it makes predictions on the training set, weights are updated, and so on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osm8e7fLK7C9"
      },
      "source": [
        "![Image](https://i.ibb.co/GTKtnG0/Boost.jpg)\n",
        "\n",
        "Figure 1: AdaBoost sequential training with instance weight updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjSKIefJr_bv"
      },
      "source": [
        "#### Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leiyBlcNr_bv"
      },
      "source": [
        "Gradient boosting is known to be one of the leading ensemble algorithms. Gradient boosting algorithm uses gradient descent method to optimize the loss function.\n",
        "\n",
        "Just  like  AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration just like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
        "\n",
        "A simpler way to train the gradient boosting regression trees ensemble technique is to use Scikit-Learn’s Gradient Boosting Regressor class. Much like the Random Forest Regressor class, it has hyperparameters to control the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on), as  well  as  hyperparameters  to  control  the  ensemble  training,  such  as  the  number  of trees (n_estimators)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hbIpjeqr_bv"
      },
      "source": [
        "##### How boosting is accomplished?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uofXYxLr_bv"
      },
      "source": [
        "- Iteratively learning a set of weak models on subsets of the data\n",
        "- Weighting each weak prediction according to each weak learner's performance\n",
        "- Combine the weighted predictions to obtain a single weighted prediction\n",
        "- that is much better than the individual predictions themselves!\n",
        "\n",
        "\n",
        "Here is a list of essential components required by Gradient Boosting Algorithms:\n",
        "\n",
        "**Additive Model**\n",
        "\n",
        "We try to minimize losses by implementing more decision trees.  We can also diminish the error rates by minimizing the parameters. In cases like these, we create the model to ensure there are no changes to the existing tree despite the addition of another one.\n",
        "\n",
        "**Weak Learner**\n",
        "\n",
        "Weak learners are an essential part of gradient boosting for making predictions. We utilize regression trees to extract authentic values. It is essential to develop trees greedily to arrive at the most favorable split point. It is a significant reason why the model mostly overfits the specific dataset.\n",
        "\n",
        "**Loss Function**\n",
        "\n",
        "We must optimize loss functions to reduce prediction-related errors. Contrary to Ada Boost, the wrong result does not receive an increased weight in gradient boosting. Instead, it minimizes the loss function from weak learners by obtaining output averages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_06_Boosting_Stacking_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/indian_liver_patient.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCa1915Wr_bl"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM64Mh9rr_bl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Import support vector regressor algorithm\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "# Import modelling methods\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
        "# Import the model performance evaluation metrics\n",
        "from sklearn import metrics\n",
        "# Import Adaboost, Gradient Boost, Random Forest and Stacking algorithm\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor, RandomForestRegressor, StackingRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.datasets import fetch_california_housing  # to import california housing dataset\n",
        "# to visualize decision boundaries\n",
        "import graphviz\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DVZkjZ5Cg6M"
      },
      "source": [
        "#### Define the AdaBoost classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqdnNe1DWRXg"
      },
      "source": [
        "Let’s take a closer look at the AdaBoost algorithm. Each instance weight $w^{(i)}$ is initially set to $\\frac{1}{m}$. A first predictor is trained and its weighted error rate $r_1$ is computed on the training set\n",
        "\n",
        "*Equation 1: Weighted error rate of the $j^{th}$ predictor*\n",
        "\n",
        "$r_j =$ $\\frac {\\underset{\\hat y_j^i \\neq y^i}{\\sum_{i=1}^{m} w^i}}{\\sum_{i=1}^{m} w^i}$\n",
        "\n",
        "where $\\hat{y_j^i}$ is  the $j^{th}$ predictor’s  prediction  for  the $i^{th}$ instance.\n",
        "\n",
        "The predictor’s weight $α_j$ is then computed using Equation 2, where $η$ is the learning  rate  hyperparameter. The  more  accurate  the  predictor  is,  the higher its weight will be. If it is just guessing randomly, then its weight will be close to zero.  However,  if  it  is  most  often  wrong  (i.e.,  less  accurate  than  random  guessing), then its weight will be negative.\n",
        "\n",
        "*Equation 2: Predictor weight*\n",
        "\n",
        "$\\alpha_j =$ $\\eta \\log \\frac{1-r_j}{r_J}$\n",
        "\n",
        "Next, the instance weights are updated using Equation 3: the misclassified instances are boosted.\n",
        "\n",
        "*Equation 3: Weight update rule*\n",
        "\n",
        "for $i= 1, 2,⋯,m$\n",
        "\n",
        "\\begin{align}\n",
        "w^{(i)} \\leftarrow \\left\\{ \\begin{array}{cc}\n",
        "                w^{(i)} & \\hspace{5mm} if \\hspace{2.5mm} \\hat y_j^{(i)} = y^{(i)} \\\\\n",
        "                w^{(i)} \\exp(a_j) & \\hspace{5mm} if \\hspace{2.5mm} \\hat y_j^{(i)} \\neq y^{(i)}  \\\\\n",
        "                \\end{array} \\right.\n",
        "\\end{align}\n",
        "\n",
        "Then all the instance weights are normalized (i.e., divided by $∑_{i=1}^{m} w_i$).\n",
        "\n",
        "Finally, a new predictor is trained using the updated weights, and the whole process is repeated  (the  new  predictor’s  weight  is  computed,  the  instance  weights  are  updated, then  another  predictor  is  trained,  and  so  on).  The  algorithm  stops  when  the  desired number of predictors is reached, or when a perfect predictor is found. To make predictions, AdaBoost simply computes the predictions of all the predictors and  weighs  them  using  the  predictor  weights  $α_j$.  The  predicted  class  is  the  one  that receives the majority of weighted votes (see Equation 4).\n",
        "\n",
        "*Equation 4: AdaBoost predictions*\n",
        "\n",
        "$\\hat y(X) = \\underset{k}{\\operatorname{argmax}} \\underset{\\hat y_j(X) = k}{\\operatorname{\\sum_{j=1}^{N} \\alpha_j}} \\hspace{1cm}$ Where $N$ is the number of predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzafWg5Hr_bo"
      },
      "source": [
        "In the following exercises, we will use the [Indian Liver Patient](https://www.kaggle.com/uciml/indian-liver-patient-records) dataset. The task is to predict whether a patient suffers from a liver disease using 10 features including Albumin, age, and gender. We will be training an AdaBoost ensemble to perform the classification task. In addition, given that this dataset is imbalanced, we will be using the ROC AUC score as a metric instead of accuracy.\n",
        "\n",
        "As a first step, we will start by instantiating an AdaBoost classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zopN_4OGr_bo"
      },
      "source": [
        "df = pd.read_csv('indian_liver_patient.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHfXwBEOr_bo"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS3a71p_r_bo"
      },
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7bs8RBar_bo"
      },
      "source": [
        "# Drop missing values\n",
        "df1 = df.dropna()\n",
        "df1.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knf7Rncjr_bo"
      },
      "source": [
        "Now, we plot a correlation matrix and see how the attributes are correlated to each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndZ8GjVdr_bp"
      },
      "source": [
        "# Visualize correlation matrix\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "numeric_df1 = df1.select_dtypes(include='number')\n",
        "sns.heatmap(abs(numeric_df1.corr()), annot=True, square=True, cbar=False, ax=ax, linewidths=0.25);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnkiXJ7Xr_bp"
      },
      "source": [
        "# Drop correlated features\n",
        "df2 = df1.drop(columns= ['Direct_Bilirubin', 'Alamine_Aminotransferase', 'Total_Protiens'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt8npiDlr_bp"
      },
      "source": [
        "As we can see there is a correlation between\n",
        "\n",
        "- *Total_Bilirubin* and *Direct_Bilirubin*\n",
        "\n",
        "- *Alamine_Aminotransferase* and ^Aspartate_Aminotransferase*\n",
        "\n",
        "- *Total_Protiens* and *Albumin*\n",
        "\n",
        "The features we drop from data are: *Direct_Bilirubin*, *Alamine_Aminotransferase* and *Total_Protiens*.\n",
        "\n",
        "There is only one column *Gender* of categorical type and remaining below are continuous data.\n",
        "\n",
        "- Age\n",
        "- Total_Bilirubin\n",
        "- Alkaline_Phosphotase\n",
        "- Aspartate_Aminotransferase\n",
        "- Albumin\n",
        "- Albumin_and_Globulin_Ratio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJdeAT1Br_bp"
      },
      "source": [
        "df2['Dataset'] = df2['Dataset'].replace(1,0)\n",
        "df2['Dataset'] = df2['Dataset'].replace(2,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcx7JIYpr_bp"
      },
      "source": [
        "Now, we will segregate the disease according to gender and total people who participated in this study."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s69BU1Zqr_bp"
      },
      "source": [
        "print('How many people have disease:', '\\n', df2.groupby('Gender')[['Dataset']].sum(), '\\n')\n",
        "print('How many people participated in the study:', '\\n', df2.groupby('Gender')[['Dataset']].count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrsiu5lUr_bq"
      },
      "source": [
        "Now, we will see the percentage of male and female having disease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcLXMY_Qr_bq"
      },
      "source": [
        "print('Percentage of people with the disease depending on gender:')\n",
        "df2.groupby('Gender')[['Dataset']].sum()/ df2.groupby('Gender')[['Dataset']].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpKIuEKIr_bq"
      },
      "source": [
        "Women have a higher percentage of the disease, so we will conduct a separate study, depending on the gender of the person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zsnVmKir_br"
      },
      "source": [
        "# defining the X and y variables\n",
        "X = df2[['Gender', 'Total_Bilirubin','Alkaline_Phosphotase','Aspartate_Aminotransferase','Albumin','Albumin_and_Globulin_Ratio']]\n",
        "y = pd.Series(df2['Dataset'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umb7qIhyr_br"
      },
      "source": [
        "We will apply label encoding for categorical data (Gender)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pohAZrMJr_br"
      },
      "source": [
        "labelencoder = LabelEncoder()\n",
        "X['Gender'] = labelencoder.fit_transform(X['Gender'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap3OofyMr_br"
      },
      "source": [
        "Now, we will split the data into training and testing datasets. After, that we scale the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7bIdXlCr_bs"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(x_train)\n",
        "X_test = scaler.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0ABUzRDr_bs"
      },
      "source": [
        "**Train the model Using AdaBoost Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjiyCmb5r_bs"
      },
      "source": [
        "ADB = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),\n",
        "                         n_estimators=125,\n",
        "                         learning_rate = 0.6,\n",
        "                         random_state=42)\n",
        "\n",
        "ADB.fit(X_train, y_train)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLgJ3jD7r_bs"
      },
      "source": [
        "Repeated k-fold cross-validation provides a way to improve the estimated performance of a machine learning model. This involves simply repeating the cross-validation procedure multiple times and reporting the mean result across all folds from all runs. This mean result is expected to be a more accurate estimate of the true unknown underlying mean performance of the model on the dataset, as calculated using the standard error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D0i0xuyr_bs"
      },
      "source": [
        "# calculating model evaluation metrics using cross_val_score like accuracy, R2 score, etc.\n",
        "n_scores = cross_val_score(ADB, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "('Accuracy: %.3f' % (np.mean(n_scores)*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMwoM4Yqr_bs"
      },
      "source": [
        "#### Create a Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoKJ0m4Ar_bs"
      },
      "source": [
        "labels = ADB.predict(X_test)\n",
        "matrix = metrics.confusion_matrix(y_test, labels)\n",
        "# creating a heat map to visualize confusion matrix\n",
        "sns.heatmap(matrix.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrHytPgCr_bt"
      },
      "source": [
        "From the above plot of the confusion matrix, we can say that the AdaBoost classifier model predicts 56 'no disease' and 13 'with disease' values correctly.\n",
        "\n",
        "Now, we plot ROC and AUC curve for the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9IwOru6r_bt"
      },
      "source": [
        "logit_roc_auc = metrics.roc_auc_score(y_test, labels)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, ADB.predict_proba(X_test)[:,1])\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='(area = %0.2f)' % logit_roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('Log_ROC')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCMQGqZrr_bt"
      },
      "source": [
        "So, we get an Area under the curve value (AUC) of 0.53.\n",
        "\n",
        "When we need to check or visualize the performance of the multi-class classification problem, we use the AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve. It is one of the most important evaluation metrics for checking any classification model’s performance.\n",
        "\n",
        "To know more about AUC-ROC, click [here](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht9CbvSzr_bw"
      },
      "source": [
        "#### Gradient Boosting Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2cHq00Xr_bw"
      },
      "source": [
        "We use California house-price dataset as a regression dataset in this example. After loading the dataset, first, we will separate data into x and y parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8j1--rzr_b1"
      },
      "source": [
        "california = fetch_california_housing()\n",
        "print(california.keys())\n",
        "print(\"shape of dataset\",california.data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpCZt58nr_b1"
      },
      "source": [
        "As we can see it returned (20640, 8), that means there are 20640 rows of data with 8 columns. Now, if we want to know what the 8 columns are, we can simply use the `.feature_names` attribute and it will return the feature names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ-XLNOWr_b1"
      },
      "source": [
        "print(california.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tx2zwe9r_b2"
      },
      "source": [
        "Now let’s convert it into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEjhqL4vr_b2"
      },
      "source": [
        "df = pd.DataFrame(california.data)\n",
        "df.columns = california.feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sectWmNtr_b2"
      },
      "source": [
        "Explore the top 5 rows of the dataset by using head() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vix8v4jCr_b2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_a1FcgBr_b2"
      },
      "source": [
        "Here, we will notice that there is no column called `PRICE` in the DataFrame. This is because the target column is available in another attribute called `california.target`. Append `california.target` to the pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbn0XIq0r_b2"
      },
      "source": [
        "df['PRICE'] = california.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsXBDviLr_b3"
      },
      "source": [
        "Now, we will run the `.info()` method on our DataFrame to get useful information about the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waoDNgu6r_b3"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcvSrs1ur_b3"
      },
      "source": [
        "Separate the target variable and rest of the variables using .iloc to subset the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXQKXnZ9r_b3"
      },
      "source": [
        "X, y = df.iloc[:,:-1],df.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z00bMAkr_bx"
      },
      "source": [
        "xtrain, xtest, ytrain, ytest=train_test_split(X, y, random_state=12, test_size=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3PxKn8er_bx"
      },
      "source": [
        "Defining the model\n",
        "\n",
        "We can define the model with its default parameters or set the new parameter values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0qqV_vNr_bx"
      },
      "source": [
        "# with new parameters\n",
        "gbr1 = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', n_estimators=600,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.01,\n",
        "    min_samples_split=4)\n",
        "# with default parameters\n",
        "gbr = GradientBoostingRegressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL6AbVVXx_bi"
      },
      "source": [
        "Fit the model with default parameters and predict the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq3SUQbgx-sR"
      },
      "source": [
        "# fit with default parameters\n",
        "gbr.fit(xtrain, ytrain)\n",
        "\n",
        "ypred = gbr.predict(xtest)\n",
        "\n",
        "# calculating Mean Squared Error\n",
        "mse = metrics.mean_squared_error(ytest,ypred)\n",
        "# mse for default model\n",
        "print(\"MSE: %.2f\" % mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBFxnVEdyFcO"
      },
      "source": [
        "Fit the model by passing parameters and predict the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quGc7ldNr_bz"
      },
      "source": [
        "# fit by passing hyperparameters\n",
        "gbr1.fit(xtrain, ytrain)\n",
        "\n",
        "ypred1 = gbr1.predict(xtest)\n",
        "# calculating Mean Squared Error\n",
        "mse1 = metrics.mean_squared_error(ytest, ypred1)\n",
        "\n",
        "# mse for regularized model\n",
        "print(\"MSE: %.2f\" % mse1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej74dBcjr_bz"
      },
      "source": [
        "Finally, we will visualize the actual and predicted values in a plot for both models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM6PAmoVr_b0"
      },
      "source": [
        "x_ax = range(len(ytest))\n",
        "plt.scatter(x_ax, ytest, s=5, color=\"blue\", label=\"original\")\n",
        "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIzwDA6er_b0"
      },
      "source": [
        "x_ax = range(len(ytest))\n",
        "plt.scatter(x_ax, ytest, s=5, color=\"blue\", label=\"original\")\n",
        "plt.plot(x_ax, ypred1, lw=0.8, color=\"red\", label=\"predicted\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoosPDP7r_b0"
      },
      "source": [
        "From the above plots, we can see that the mean square error is less for the regularized model than the default model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icDeoysxr_b0"
      },
      "source": [
        "#### Implementation of XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URXF2Hkfr_b0"
      },
      "source": [
        "An optimized implementation of Gradient Boosting is available in the popular python library `XGBoost`, which stands for Extreme Gradient Boosting. This package aims at being extremely fast, scalable, and  portable.  In  fact,  XGBoost  is  often  an  important  component  of winning entries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKhbrmder_b0"
      },
      "source": [
        "Here, we also going to import the California Housing dataset and store it in a variable called california."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CNUH_Phr_b4"
      },
      "source": [
        "The next step is to instantiate an `XGBoost regressor` object by calling the `XGBRegressor()` class from the `XGBoost` library with the hyper-parameters passed as arguments. For classification problems, we would have used the `XGBClassifier()` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdvy5YyLr_b4"
      },
      "source": [
        "xgb_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
        "                           max_depth = 5, alpha = 10, n_estimators = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OcL3DCBr_b4"
      },
      "source": [
        "Now, we will create the train and test set for cross-validation of the results using the `train_test_split` function from sklearn's `model_selection` module with `test_size` size equal to 20% of the data. Also, to maintain the reproducibility of the results, a random_state is also assigned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCJGLQp8r_b4"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXbXD4ajr_b4"
      },
      "source": [
        "xgb_reg.fit(X_train,y_train)\n",
        "\n",
        "y_pred = xgb_reg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEoJIeBRr_b4"
      },
      "source": [
        "mse2 = metrics.mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE: %f\" % (mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csyq2VzVr_b5"
      },
      "source": [
        "Plotting the `feature importance` graph with the matplotlib library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2yjvjKcr_b5"
      },
      "source": [
        "xgb.plot_importance(xgb_reg)\n",
        "plt.rcParams['figure.figsize'] = [5, 5]\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0n-cTAHr_b5"
      },
      "source": [
        "As we can see the feature `MedInc` and `AveRooms` have been given the highest importance score among all the features. Thus XGBoost also gives us a way to do Feature Selection.\n",
        "\n",
        "To know more about XGboost, click [here](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module2/M2_AST_06_Boosting_Stacking_XGBoost.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5OTp81Or_b5"
      },
      "source": [
        "### Stacking  (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myiOG4QasoTU"
      },
      "source": [
        "The overall idea of the stacking ensemble method is to train several models with different algorithm types (i.e. base-learners), on the train data, and then aggregate all the models using another model (meta learner), to make the final prediction. The inputs for the meta-learner are the prediction outputs of the base-learners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-INouAOCr_b6"
      },
      "source": [
        "![Image](https://miro.medium.com/max/2078/1*Hyd8g0mdXeTAqwt-Ghak3w.png)\n",
        "\n",
        "Figure 2: Predictions in a multilayer stacking ensemble\n",
        "\n",
        "For a given input data point, we pass it through the M base-learners and get M number of predictions, and send those M predictions through the meta-learner as inputs and obtain the final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGUhiG6rr_b6"
      },
      "source": [
        "Here, we again look at the California housing dataset and try to build a regressor model using the stacking method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuBkufxYr_b8"
      },
      "source": [
        "**Stacking Models**\n",
        "\n",
        "We assume no hyperparameters for the stacking – this means that we use the predefined hyperparameters for each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exfWePgNr_b9"
      },
      "source": [
        "xgb = XGBRegressor()\n",
        "rf = RandomForestRegressor(n_estimators=400, max_depth=5, max_features=6)\n",
        "ridge = Ridge()\n",
        "lasso = Lasso()\n",
        "svr = SVR(kernel='rbf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM0ptma3r_b9"
      },
      "source": [
        "Now that we have defined all our models, we can begin improving our results by stacking some models. As we can see here, we defined two levels, where the first level has 5 models, and the second level has the meta-learner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jcek4tlr_b9"
      },
      "source": [
        "estimators = [('ridge', ridge), ('svr', svr), ('rf', rf), ('lasso', lasso)]\n",
        "reg = StackingRegressor(estimators=estimators,final_estimator=xgb)\n",
        "# fit the model\n",
        "reg.fit(X_train, y_train)\n",
        "pred = reg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQbNOJNor_b9"
      },
      "source": [
        "score = metrics.r2_score(y_test, pred)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sztdeYObr_b9"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L01s7i1Hr_b-"
      },
      "source": [
        "1. If  your  AdaBoost  ensemble  underfits  the  training  data,  what  hyperparameters should you tweak and how?\n",
        "\n",
        " If your AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate.\n",
        "\n",
        "2. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
        "\n",
        " If your Gradient Boosting ensemble overfits the training set, you should try decreasing the learning rate. You could also use early stopping to find the right number of predictors (you probably have too many)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Select the False Statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"Gradient boosting re-defines boosting as a numerical optimisation problem where the objective is to minimize the loss function of the model by adding weak learners using gradient descent\", \"In Gradient Boosting sequential predictors are trained on the residuals of the previous predictor\", \"XGBoost is a scalable and accurate implementation of the adaptive boosting technique\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}